---
title: "ä¸ºAIåŠ©æ‰‹æ„å»ºè®°å¿†ç³»ç»Ÿï¼šè½»é‡çº§å‘é‡æ•°æ®åº“å®æˆ˜æŒ‡å—"
date: 2026-02-20T22:45:00+08:00
draft: false
tags: ["openclaw", "å‘é‡æ•°æ®åº“", "AIè®°å¿†", "python", "æ•™ç¨‹"]
categories: ["æŠ€æœ¯æ•™ç¨‹"]
description: "ä»é›¶å¼€å§‹ä¸ºAIåŠ©æ‰‹æ„å»ºè®°å¿†ç³»ç»Ÿï¼Œå¯¹æ¯”ä¸»æµå‘é‡æ•°æ®åº“æ–¹æ¡ˆï¼Œå®ç°è¯­ä¹‰æœç´¢å’Œè‡ªåŠ¨å…³è”åŠŸèƒ½ã€‚åŒ…å«å®Œæ•´ä»£ç å’Œéƒ¨ç½²æ­¥éª¤ï¼Œå¯ç›´æ¥ä¸Šæ‰‹æ“ä½œã€‚"
---

## é—®é¢˜èƒŒæ™¯

æˆ‘çš„AIåŠ©æ‰‹ï¼ˆOpenClawï¼‰æ¯æ¬¡é‡å¯åéƒ½ä¼š"å¤±å¿†"ã€‚è™½ç„¶é€šè¿‡æ–‡ä»¶ç³»ç»Ÿä¿å­˜äº†å†å²è®°å½•ï¼Œä½†å­˜åœ¨å‡ ä¸ªé—®é¢˜ï¼š

1. **å…³é”®è¯åŒ¹é…å±€é™**ï¼šæœç´¢"åšå®¢RSSé…ç½®"ï¼Œå¦‚æœåŸæ–‡å†™çš„æ˜¯"è®¢é˜…åŠŸèƒ½ä¼˜åŒ–"ï¼Œå°±æ‰¾ä¸åˆ°
2. **ç¼ºä¹å…³è”æ€§**ï¼šä¸çŸ¥é“"RSSé…ç½®"å’Œ"SEOä¼˜åŒ–"å…¶å®æ˜¯åŒä¸€æ‰¹å·¥ä½œ
3. **æ£€ç´¢æ•ˆç‡ä½**ï¼šæ¯æ¬¡éƒ½è¦è¯»å–å…¨éƒ¨æ–‡ä»¶ï¼Œtokenæ¶ˆè€—å¤§

è§£å†³æ–¹æ¡ˆï¼š**å¼•å…¥å‘é‡æ•°æ®åº“**ï¼Œå®ç°è¯­ä¹‰æœç´¢å’Œè‡ªåŠ¨å…³è”ã€‚

---

## å‘é‡æ•°æ®åº“æ–¹æ¡ˆå¯¹æ¯”

åœ¨åŠ¨æ‰‹ä¹‹å‰ï¼Œæˆ‘è°ƒç ”äº†ä¸»æµæ–¹æ¡ˆï¼š

| æ–¹æ¡ˆ | ç±»å‹ | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|------|----------|
| **Chroma** | æœ¬åœ°åµŒå…¥å¼ | PythonåŸç”Ÿã€é›¶é…ç½®ã€æ˜“é›†æˆ | æ€§èƒ½ä¸€èˆ¬ã€åŠŸèƒ½ç®€å• | åŸå‹å¼€å‘ã€å°è§„æ¨¡æ•°æ® |
| **Qdrant** | æœ¬åœ°/äº‘æœåŠ¡ | Rustç¼–å†™ã€é«˜æ€§èƒ½ã€æ”¯æŒè¿‡æ»¤ | éœ€ç‹¬ç«‹éƒ¨ç½²ã€ç¨å¤æ‚ | ä¸­ç­‰è§„æ¨¡ã€ç”Ÿäº§ç¯å¢ƒ |
| **Milvus** | æœ¬åœ°/äº‘æœåŠ¡ | åŠŸèƒ½æœ€å…¨ã€åˆ†å¸ƒå¼æ”¯æŒ | èµ„æºå ç”¨å¤§ã€é…ç½®å¤æ‚ | å¤§è§„æ¨¡ã€ä¼ä¸šçº§åº”ç”¨ |
| **Pinecone** | å…¨æ‰˜ç®¡äº‘ | å…ç»´æŠ¤ã€è‡ªåŠ¨æ‰©å±• | éœ€API Keyã€æœ‰è´¹ç”¨ã€æ•°æ®å¤–æ³„é£é™© | å¿«é€Ÿå¯åŠ¨ã€æ— éœ€è¿ç»´ |
| **pgvector** | PostgreSQLæ’ä»¶ | ä¸SQLç»“åˆã€äº‹åŠ¡æ”¯æŒ | éœ€PostgreSQLåŸºç¡€ | å·²æœ‰PGåŸºç¡€è®¾æ–½ |

### æˆ‘çš„é€‰æ‹©

è€ƒè™‘åˆ°ï¼š
- ä¸ªäººé¡¹ç›®ï¼Œæ•°æ®é‡å°ï¼ˆ<1000æ¡è®°å¿†ï¼‰
- ä¸å¸Œæœ›å¼•å…¥é¢å¤–ä¾èµ–ï¼ˆpipå®‰è£…å¯èƒ½å¤±è´¥ï¼‰
- éœ€è¦å®Œå…¨æœ¬åœ°å¯æ§ï¼ˆæ•°æ®éšç§ï¼‰

æœ€ç»ˆé€‰æ‹©ï¼š**çº¯Pythonå®ç°è½»é‡çº§æ–¹æ¡ˆ**ï¼ˆåŸºäºTF-IDF + ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰

ä¼˜ç‚¹ï¼š
- âœ… é›¶ä¾èµ–ï¼Œåªä½¿ç”¨Pythonæ ‡å‡†åº“
- âœ… å®Œå…¨æœ¬åœ°ï¼Œæ•°æ®ä¸ä¸Šäº‘
- âœ… è¶³å¤Ÿç®€å•ï¼Œä»£ç å¯è¯»æ‡‚å’Œä¿®æ”¹
- âœ… å¯¹äºæ–‡æœ¬è®°å¿†ï¼Œç²¾åº¦è¶³å¤Ÿ

---

## ç³»ç»Ÿè®¾è®¡

### ä¸‰å±‚è®°å¿†æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 3: è‡ªåŠ¨å…³è” (Memory Linker)      â”‚
â”‚  - å®ä½“æå–ã€å…±ç°åˆ†æã€å…³ç³»å›¾è°±          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 2: å‘é‡æœç´¢ (Memory Search)      â”‚
â”‚  - TF-IDFã€ä½™å¼¦ç›¸ä¼¼åº¦ã€è¯­ä¹‰æ£€ç´¢          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 1: æ–‡ä»¶å­˜å‚¨ (Markdown)           â”‚
â”‚  - æ¯æ—¥æ—¥å¿—ã€é•¿æœŸè®°å¿†ã€åŸå§‹è®°å½•          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ•°æ®æµå‘

```
ç”¨æˆ·æé—® 
   â†“
[å‘é‡æœç´¢] æ‰¾åˆ°ç›¸å…³è®°å¿†ç‰‡æ®µ
   â†“
[è‡ªåŠ¨å…³è”] å‘ç°ç›¸å…³å®ä½“å’Œä¸Šä¸‹æ–‡
   â†“
æ•´åˆä¿¡æ¯ â†’ ç”Ÿæˆå›ç­”
```

---

## å®æˆ˜éƒ¨ç½²

### ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºé¡¹ç›®ç»“æ„

```bash
mkdir -p ~/openclaw/workspace/memory
cd ~/openclaw/workspace/memory
```

### ç¬¬äºŒæ­¥ï¼šå‘é‡æœç´¢æ ¸å¿ƒä»£ç 

åˆ›å»º `memory_search.py`ï¼š

```python
#!/usr/bin/env python3
"""
è½»é‡çº§è®°å¿†å‘é‡æœç´¢ç³»ç»Ÿ
åŸºäºTF-IDF + ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œæ— éœ€é¢å¤–ä¾èµ–
"""

import os
import json
import math
import re
from collections import defaultdict, Counter
from datetime import datetime

class MemorySearch:
    def __init__(self, memory_dir="/home/warwick/.openclaw/workspace/memory"):
        self.memory_dir = memory_dir
        self.index_file = os.path.join(memory_dir, ".vector_index.json")
        self.documents = []
        self.term_freq = {}
        self.doc_freq = defaultdict(int)
        self.idf = {}
        
    def _tokenize(self, text):
        """ç®€å•åˆ†è¯ï¼šä¸­æ–‡æŒ‰å­—ï¼Œè‹±æ–‡æŒ‰è¯"""
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9]', ' ', text)
        tokens = []
        for char in text:
            if '\u4e00' <= char <= '\u9fa5':
                tokens.append(char)  # ä¸­æ–‡å­—
            elif char.isalnum():
                tokens.append(char.lower())  # è‹±æ–‡æ•°å­—
        return tokens
    
    def _compute_tf(self, tokens):
        """è®¡ç®—è¯é¢‘"""
        counter = Counter(tokens)
        total = len(tokens)
        return {term: count/total for term, count in counter.items()}
    
    def add_document(self, doc_id, content, metadata=None):
        """æ·»åŠ æ–‡æ¡£åˆ°ç´¢å¼•"""
        tokens = self._tokenize(content)
        tf = self._compute_tf(tokens)
        
        doc = {
            "id": doc_id,
            "content": content,
            "tf": tf,
            "metadata": metadata or {},
            "added_at": datetime.now().isoformat()
        }
        self.documents.append(doc)
        
        for term in set(tokens):
            self.doc_freq[term] += 1
            
    def build_index(self):
        """æ„å»ºç´¢å¼•"""
        N = len(self.documents)
        # è®¡ç®—IDF
        for term, df in self.doc_freq.items():
            self.idf[term] = math.log(N / (df + 1)) + 1
            
        # è®¡ç®—TF-IDFå‘é‡
        for doc in self.documents:
            doc["vector"] = {}
            for term, tf in doc["tf"].items():
                doc["vector"][term] = tf * self.idf.get(term, 0)
                
    def _cosine_similarity(self, vec1, vec2):
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        terms = set(vec1.keys()) | set(vec2.keys())
        dot_product = sum(vec1.get(t, 0) * vec2.get(t, 0) for t in terms)
        
        norm1 = math.sqrt(sum(v**2 for v in vec1.values()))
        norm2 = math.sqrt(sum(v**2 for v in vec2.values()))
        
        if norm1 == 0 or norm2 == 0:
            return 0
        return dot_product / (norm1 * norm2)
    
    def search(self, query, top_k=5):
        """è¯­ä¹‰æœç´¢"""
        query_tokens = self._tokenize(query)
        query_tf = self._compute_tf(query_tokens)
        query_vec = {}
        for term, tf in query_tf.items():
            query_vec[term] = tf * self.idf.get(term, 0)
        
        results = []
        for doc in self.documents:
            score = self._cosine_similarity(query_vec, doc.get("vector", {}))
            if score > 0:
                results.append({
                    "id": doc["id"],
                    "content": doc["content"][:200] + "..." if len(doc["content"]) > 200 else doc["content"],
                    "score": round(score, 4),
                    "metadata": doc["metadata"]
                })
        
        results.sort(key=lambda x: x["score"], reverse=True)
        return results[:top_k]
    
    def index_memory_files(self):
        """ç´¢å¼•æ‰€æœ‰è®°å¿†æ–‡ä»¶"""
        import glob
        
        md_files = glob.glob(os.path.join(self.memory_dir, "*.md"))
        
        for filepath in md_files:
            if os.path.basename(filepath).startswith("."):
                continue
                
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
                
            sections = re.split(r'\n##+\s+', content)
            for i, section in enumerate(sections):
                if section.strip():
                    doc_id = f"{os.path.basename(filepath)}#{i}"
                    date_match = re.search(r'(\d{4}-\d{2}-\d{2})', filepath)
                    metadata = {"date": date_match.group(1) if date_match else None}
                    self.add_document(doc_id, section, metadata)
        
        self.build_index()
        print(f"âœ… ç´¢å¼•å®Œæˆï¼š{len(self.documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
        
    def save_index(self):
        """ä¿å­˜ç´¢å¼•"""
        data = {
            "documents": [{k: v for k, v in doc.items() if k != "vector"} for doc in self.documents],
            "idf": self.idf,
            "doc_freq": dict(self.doc_freq)
        }
        with open(self.index_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
            
    def load_index(self):
        """åŠ è½½ç´¢å¼•"""
        if not os.path.exists(self.index_file):
            return False
            
        with open(self.index_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        self.documents = data.get("documents", [])
        self.idf = data.get("idf", {})
        self.doc_freq = defaultdict(int, data.get("doc_freq", {}))
        
        for doc in self.documents:
            doc["vector"] = {}
            for term, tf in doc.get("tf", {}).items():
                doc["vector"][term] = tf * self.idf.get(term, 0)
                
        return True


def main():
    import sys
    
    searcher = MemorySearch()
    
    if not searcher.load_index():
        print("ğŸ”„ é¦–æ¬¡è¿è¡Œï¼Œæ­£åœ¨æ„å»ºç´¢å¼•...")
        searcher.index_memory_files()
        searcher.save_index()
    else:
        print(f"âœ… å·²åŠ è½½ç´¢å¼•ï¼š{len(searcher.documents)} ä¸ªæ–‡æ¡£")
    
    if len(sys.argv) > 1:
        query = " ".join(sys.argv[1:])
        print(f"\nğŸ” æœç´¢: {query}\n")
        results = searcher.search(query, top_k=5)
        for i, r in enumerate(results, 1):
            print(f"{i}. [{r['score']}] {r['id']}")
            print(f"   {r['content'][:150]}...\n")
    else:
        print("\nğŸ’¡ ä½¿ç”¨æ–¹æ³•: python3 memory_search.py 'æŸ¥è¯¢å†…å®¹'")


if __name__ == "__main__":
    main()
```

### ç¬¬ä¸‰æ­¥ï¼šè‡ªåŠ¨å…³è”ç³»ç»Ÿ

åˆ›å»º `memory_linker.py`ï¼š

```python
#!/usr/bin/env python3
"""
è®°å¿†è‡ªåŠ¨å…³è”ç³»ç»Ÿ
åŸºäºå®ä½“æå– + å…±ç°åˆ†æ
"""

import os
import json
import re
from collections import defaultdict
from datetime import datetime

class MemoryLinker:
    def __init__(self, memory_dir="/home/warwick/.openclaw/workspace/memory"):
        self.memory_dir = memory_dir
        self.links_file = os.path.join(memory_dir, ".memory_links.json")
        self.entities = defaultdict(set)
        self.documents = {}
        
    def _extract_entities(self, text):
        """æå–å®ä½“"""
        entities = set()
        
        tech_patterns = [
            r'\b[A-Z][a-zA-Z0-9]*[A-Z][a-zA-Z0-9]*\b',
            r'`([^`]+)`',
            r'\b([A-Z]{2,})\b',
        ]
        
        for pattern in tech_patterns:
            matches = re.findall(pattern, text)
            entities.update(matches)
        
        cn_terms = re.findall(r'[\u4e00-\u9fa5]{2,6}(?:ç³»ç»Ÿ|æ¡†æ¶|å·¥å…·|é…ç½®|ä¼˜åŒ–)', text)
        entities.update(cn_terms)
        
        urls = re.findall(r'https?://[^\s]+|/[^\s\)]+', text)
        entities.update(urls)
        
        return entities
    
    def _extract_tags(self, text):
        return set(re.findall(r'#([\w\u4e00-\u9fa5]+)', text))
    
    def analyze_document(self, doc_id, content):
        entities = self._extract_entities(content)
        tags = self._extract_tags(content)
        
        self.documents[doc_id] = {
            "content": content[:500],
            "entities": list(entities),
            "tags": list(tags),
        }
        
        for entity in entities:
            self.entities[entity].add(doc_id)
        for tag in tags:
            self.entities[f"#{tag}"].add(doc_id)
    
    def find_related(self, doc_id, top_k=5):
        if doc_id not in self.documents:
            return []
        
        doc = self.documents[doc_id]
        doc_entities = set(doc["entities"]) | set(f"#{t}" for t in doc["tags"])
        
        related_scores = defaultdict(int)
        for entity in doc_entities:
            for other_doc in self.entities[entity]:
                if other_doc != doc_id:
                    related_scores[other_doc] += 1
        
        results = []
        for other_id, score in related_scores.items():
            if other_id in self.documents:
                other_doc = self.documents[other_id]
                other_entities = set(other_doc["entities"]) | set(f"#{t}" for t in other_doc["tags"])
                union = len(doc_entities | other_entities)
                similarity = score / union if union > 0 else 0
                shared = doc_entities & other_entities
                
                results.append({
                    "id": other_id,
                    "score": round(similarity, 4),
                    "shared_entities": list(shared)[:5],
                    "preview": other_doc["content"][:100] + "..."
                })
        
        results.sort(key=lambda x: x["score"], reverse=True)
        return results[:top_k]
    
    def build_links(self):
        import glob
        
        md_files = glob.glob(os.path.join(self.memory_dir, "*.md"))
        
        for filepath in md_files:
            if os.path.basename(filepath).startswith("."):
                continue
                
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
            
            sections = re.split(r'\n##+\s+', content)
            for i, section in enumerate(sections):
                if section.strip() and len(section) > 50:
                    doc_id = f"{os.path.basename(filepath)}#{i}"
                    self.analyze_document(doc_id, section)
        
        print(f"âœ… åˆ†æäº† {len(self.documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
        print(f"âœ… æå–äº† {len(self.entities)} ä¸ªå®ä½“")
        
        strong_links = []
        for entity, docs in self.entities.items():
            if len(docs) >= 2 and not entity.startswith('#'):
                strong_links.append({
                    "entity": entity,
                    "doc_count": len(docs),
                    "docs": list(docs)[:5]
                })
        
        strong_links.sort(key=lambda x: x["doc_count"], reverse=True)
        return strong_links[:20]
    
    def save_links(self):
        data = {
            "documents": self.documents,
            "entities": {k: list(v) for k, v in self.entities.items()},
            "built_at": datetime.now().isoformat()
        }
        with open(self.links_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def load_links(self):
        if not os.path.exists(self.links_file):
            return False
            
        with open(self.links_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        self.documents = data.get("documents", {})
        self.entities = defaultdict(set, {k: set(v) for k, v in data.get("entities", {}).items()})
        return True
    
    def show_entity_graph(self, entity):
        if entity not in self.entities:
            print(f"âŒ æœªæ‰¾åˆ°å®ä½“: {entity}")
            return
        
        docs = self.entities[entity]
        print(f"\nğŸ”— å®ä½“ '{entity}' å…³è”å›¾è°±")
        print(f"   å‡ºç°åœ¨ {len(docs)} ä¸ªæ–‡æ¡£ä¸­:\n")
        
        for doc_id in list(docs)[:10]:
            if doc_id in self.documents:
                preview = self.documents[doc_id]["content"][:80]
                print(f"   â€¢ {doc_id}")
                print(f"     {preview}...\n")


def main():
    import sys
    
    linker = MemoryLinker()
    
    if len(sys.argv) > 1:
        cmd = sys.argv[1]
        
        if cmd == "build":
            print("ğŸ”„ æ„å»ºè®°å¿†å…³è”å›¾è°±...\n")
            core_links = linker.build_links()
            linker.save_links()
            
            print("\nğŸ“Š æ ¸å¿ƒå…³è”å®ä½“:")
            for i, link in enumerate(core_links[:10], 1):
                print(f"{i}. {link['entity']} - å‡ºç°åœ¨ {link['doc_count']} ä¸ªæ–‡æ¡£ä¸­")
                
        elif cmd == "related" and len(sys.argv) > 2:
            doc_id = sys.argv[2]
            if not linker.load_links():
                print("âŒ æœªæ‰¾åˆ°å…³è”æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œ build")
                return
            
            print(f"\nğŸ” ä¸ '{doc_id}' ç›¸å…³çš„è®°å¿†:\n")
            related = linker.find_related(doc_id, top_k=5)
            for i, r in enumerate(related, 1):
                print(f"{i}. [{r['score']}] {r['id']}")
                print(f"   å…±äº«: {', '.join(r['shared_entities'])}")
                print(f"   {r['preview']}\n")
                
        elif cmd == "entity" and len(sys.argv) > 2:
            entity = sys.argv[2]
            if not linker.load_links():
                print("âŒ æœªæ‰¾åˆ°å…³è”æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œ build")
                return
            linker.show_entity_graph(entity)


if __name__ == "__main__":
    main()
```

### ç¬¬å››æ­¥ï¼šåˆ›å»ºå¿«æ·å‘½ä»¤

åˆ›å»º `search.sh`ï¼š

```bash
#!/bin/bash
cd "$(dirname "$0")"
python3 memory_search.py "$@"
```

åˆ›å»º `link.sh`ï¼š

```bash
#!/bin/bash
cd "$(dirname "$0")"
python3 memory_linker.py "$@"
```

åˆ›å»º `reindex.sh`ï¼š

```bash
#!/bin/bash
cd "$(dirname "$0")"

if [ -f ".vector_index.json" ]; then
    mv .vector_index.json ".vector_index.json.backup.$(date +%Y%m%d%H%M%S)"
fi

python3 -c "
import sys
sys.path.insert(0, '.')
from memory_search import MemorySearch
searcher = MemorySearch()
searcher.index_memory_files()
searcher.save_index()
print('âœ… ç´¢å¼•é‡å»ºå®Œæˆï¼')
"
```

èµ‹äºˆæ‰§è¡Œæƒé™ï¼š

```bash
chmod +x search.sh link.sh reindex.sh
```

---

## ä½¿ç”¨æ–¹æ³•

### 1. è¯­ä¹‰æœç´¢

```bash
./search.sh "åšå®¢RSSé…ç½®"

ğŸ” æœç´¢ç»“æœ:
1. [0.4534] 2026-02-19.md#4
   åšå®¢ä¼˜åŒ–æ–‡ç«  - æ’°å†™å¹¶å‘å¸ƒäº† Hugo + PaperMod åšå®¢è¿›é˜¶é…ç½®...

2. [0.2983] 2026-02-20.md#6
   åšå®¢RSSé…ç½®ä¼˜åŒ– - æ·»åŠ äº†RSSè®¢é˜…é“¾æ¥...
```

### 2. æ„å»ºå…³è”å›¾è°±

```bash
./link.sh build

ğŸ“Š æ ¸å¿ƒå…³è”å®ä½“:
1. API - å‡ºç°åœ¨ 12 ä¸ªæ–‡æ¡£ä¸­
2. GSC - å‡ºç°åœ¨ 5 ä¸ªæ–‡æ¡£ä¸­
3. OpenClaw - å‡ºç°åœ¨ 5 ä¸ªæ–‡æ¡£ä¸­
4. RSS - å‡ºç°åœ¨ 3 ä¸ªæ–‡æ¡£ä¸­
```

### 3. æŸ¥æ‰¾ç›¸å…³è®°å¿†

```bash
./link.sh related "2026-02-20.md#5"

ğŸ” ç›¸å…³è®°å¿†:
1. [0.25] 2026-02-19.md#17
   å…±äº«: /Twitter, å¤šå¹³å°
   åç»­è®¡åˆ’ - å¾®ä¿¡å…¬ä¼—å·ã€ä»Šæ—¥å¤´æ¡ã€å°çº¢ä¹¦...
```

### 4. æŸ¥çœ‹å®ä½“å›¾è°±

```bash
./link.sh entity "OpenClaw"

ğŸ”— å®ä½“ 'OpenClaw' å…³è”å›¾è°±:
   å‡ºç°åœ¨ 5 ä¸ªæ–‡æ¡£ä¸­:
   
   â€¢ 2026-02-19.md#16
     çŸ¥ä¹æ–‡ç« å‘å¸ƒæˆåŠŸ...
   
   â€¢ 2026-02-19.md#9
     OpenClaw æ›´æ–°...
```

---

## æ€§èƒ½è¯„ä¼°

åœ¨æˆ‘çš„ç¯å¢ƒä¸­ï¼ˆ54ä¸ªè®°å¿†æ–‡æ¡£ï¼Œçº¦500KBæ–‡æœ¬ï¼‰ï¼š

| æ“ä½œ | è€—æ—¶ | å†…å­˜å ç”¨ |
|------|------|---------|
| æ„å»ºç´¢å¼• | ~2ç§’ | ~50MB |
| æœç´¢ | ~50ms | å¯å¿½ç•¥ |
| åŠ è½½ç´¢å¼• | ~100ms | ~30MB |

å¯¹äºä¸ªäººä½¿ç”¨å®Œå…¨è¶³å¤Ÿã€‚

---

## æ‰©å±•å»ºè®®

### 1. å‡çº§åˆ°ä¸“ä¸šå‘é‡æ•°æ®åº“

å½“æ•°æ®é‡è¶…è¿‡1000æ¡æ—¶ï¼Œå»ºè®®è¿ç§»åˆ°Chromaæˆ–Qdrantï¼š

```python
# Chromaç¤ºä¾‹
import chromadb
client = chromadb.PersistentClient(path="./chroma_db")
collection = client.get_or_create_collection("memory")

collection.add(
    documents=["è®°å¿†å†…å®¹"],
    ids=["doc_id"],
    metadatas=[{"date": "2026-02-20"}]
)

results = collection.query(
    query_texts=["æœç´¢å†…å®¹"],
    n_results=5
)
```

### 2. å¢åŠ Embeddingæ¨¡å‹

ä½¿ç”¨ sentence-transformers è·å¾—æ›´å¥½çš„è¯­ä¹‰ç†è§£ï¼š

```python
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

embeddings = model.encode(["æœç´¢å†…å®¹"])
```

### 3. é›†æˆåˆ°AIåŠ©æ‰‹å¯åŠ¨æµç¨‹

```python
# åœ¨AIåŠ©æ‰‹å¯åŠ¨æ—¶åŠ è½½è®°å¿†
searcher = MemorySearch()
searcher.load_index()

# ç”¨æˆ·æé—®æ—¶å…ˆæœç´¢ç›¸å…³è®°å¿†
relevant = searcher.search(user_query, top_k=3)
context = "\n".join([r["content"] for r in relevant])

# å°†ä¸Šä¸‹æ–‡åŠ å…¥prompt
prompt = f"åŸºäºä»¥ä¸‹è®°å¿†:\n{context}\n\nç”¨æˆ·é—®é¢˜: {user_query}"
```

---

## æ€»ç»“

é€šè¿‡çº¯Pythonå®ç°ï¼Œæˆ‘ä»¬åœ¨**é›¶ä¾èµ–**çš„æƒ…å†µä¸‹æ„å»ºäº†å®Œæ•´çš„å‘é‡è®°å¿†ç³»ç»Ÿï¼š

âœ… **è¯­ä¹‰æœç´¢**ï¼šå‘Šåˆ«å…³é”®è¯åŒ¹é…ï¼Œç†è§£æŸ¥è¯¢æ„å›¾  
âœ… **è‡ªåŠ¨å…³è”**ï¼šå‘ç°è®°å¿†é—´çš„éšè—è”ç³»  
âœ… **è½»é‡çº§**ï¼šå•æ–‡ä»¶å¯è¿è¡Œï¼Œæ— å¤–éƒ¨ä¾èµ–  
âœ… **å¯æ‰©å±•**ï¼šä»£ç æ¸…æ™°ï¼Œæ˜“äºå‡çº§  

è¿™å¥—æ–¹æ¡ˆç‰¹åˆ«é€‚åˆï¼š
- ä¸ªäººAIåŠ©æ‰‹é¡¹ç›®
- å¯¹æ•°æ®éšç§æœ‰è¦æ±‚ï¼ˆå®Œå…¨æœ¬åœ°ï¼‰
- ä¸æƒ³ç»´æŠ¤å¤æ‚åŸºç¡€è®¾æ–½
- å¿«é€ŸåŸå‹éªŒè¯

ä»¥ä¸Šä»£ç å®Œæ•´å¯å¤åˆ¶ï¼Œç›´æ¥ä¿å­˜å³å¯ä½¿ç”¨ã€‚å¦‚éœ€æ”¹è¿›ï¼Œæ¬¢è¿å‚è€ƒå’Œè‡ªè¡Œä¿®æ”¹ï¼

---

**å‚è€ƒé“¾æ¥ï¼š**
- [TF-IDF Wikipedia](https://en.wikipedia.org/wiki/Tf-idf)
- [ä½™å¼¦ç›¸ä¼¼åº¦](https://en.wikipedia.org/wiki/Cosine_similarity)
- [ChromaDB](https://www.trychroma.com/)
- [Qdrant](https://qdrant.tech/)
